{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet을 활용한 하천 분류 모델입니다.\n",
    "학습에 사용한 데이터는 용량이 큰 관계로 아래 링크에서 다운 받을 수 있습니다.\n",
    "Data set 지정 과정에서 추가적인 경로 설정이 필요합니다.\n",
    "\n",
    "https://drive.google.com/drive/folders/1y_1BVRh3jepwuMR72lgonqlZsiKJL4Wl?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as utils\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.optim import lr_scheduler\n",
    "from torchsummary import summary\n",
    "import torchvision.datasets\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convcacul(in_chan,out_chan,padd,filter,stride,type):\n",
    "    size_out = (in_chan + 2*padd - filter)/stride + 1\n",
    "    size_in = (out_chan-1)*stride -2*padd + filter \n",
    "    if type == 'forward':\n",
    "        return size_out\n",
    "    elif type == 'back':\n",
    "        return size_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convcacul(in_chan = 0, out_chan=32, padd = 1, filter = 3, stride = 1, type = 'back')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.Resize((128,128)),transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "train_set = torchvision.datasets.ImageFolder(root ='augmentation_image', transform = trans )\n",
    "train_loader = DataLoader(dataset = train_set, batch_size = 8, shuffle = True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2137"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = torch.utils.data.random_split(train_set,[1800,337])\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_0 = 0\n",
    "cnt_1 = 0\n",
    "cnt_2 = 0\n",
    "cnt_3 = 0\n",
    "cnt_4 = 0\n",
    "\n",
    "for i in range(1800):\n",
    "    if train_data[i][1] == 0:\n",
    "        cnt_0+=1\n",
    "    elif train_data[i][1] == 1:\n",
    "        cnt_1+=1\n",
    "    elif train_data[i][1] == 2:\n",
    "        cnt_2+=1\n",
    "    elif train_data[i][1] == 3:\n",
    "        cnt_3+=1\n",
    "    elif train_data[i][1] == 4:\n",
    "        cnt_4+=1    \n",
    "        \n",
    "#print(sum(cnt_1,cnt_0,cnt_2,cnt_3,cnt_4))\n",
    "print(cnt_0,cnt_1,cnt_2,cnt_3,cnt_4)\n",
    "\n",
    "weights = torch.tensor([cnt_0,cnt_1,cnt_2,cnt_3,cnt_4], dtype= torch.float32)\n",
    "print(weights.sum())\n",
    "weights = weights.sum()/weights\n",
    "print(weights)\n",
    "weights = weights/weights.sum()\n",
    "print(weights)\n",
    "print(weights.sum())\n",
    "\n",
    "                       \n",
    "class_weights = torch.FloatTensor(weights).cuda()\n",
    "\n",
    "class_weights.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.FloatTensor([0.2219, 0.1552, 0.1096, 0.1560, 0.3573]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,stride = 1, down_sample = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels,kernel_size =3, stride = stride, padding = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "        \n",
    "        if down_sample == True:\n",
    "            self.down_sample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels = in_channels, out_channels= out_channels, kernel_size=1, stride= stride),\n",
    "                nn.BatchNorm2d(num_features= out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.down_sample= None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        shortcut = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            shortcut = self.down_sample(shortcut)\n",
    "\n",
    "        x += shortcut\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers, block, num_classes=5):\n",
    "        super().__init__()\n",
    "        # input img : [3, 128, 128]\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16,\n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # feature map size = [16,32,32] 16 128 128\n",
    "        self.layers_2n = self.get_layers(block, 16, 16, stride=1)\n",
    "        # feature map size = [32,16,16] 32 64 64\n",
    "        self.layers_4n = self.get_layers(block, 16, 32, stride=2)\n",
    "        # feature map size = [64,8,8]  64 32 32\n",
    "        self.layers_6n = self.get_layers(block, 32, 64, stride=2)\n",
    "        # feature map size = [64,8,8]  128 16 16\n",
    "        self.layers_8n = self.get_layers(block, 64, 128, stride=2)\n",
    "        # feature map size = [64,8,8]  256 8 8\n",
    "        self.layers_10n = self.get_layers(block, 128, 256, stride=2)\n",
    "####################################################################################################################################\n",
    "        # output layers\n",
    "        self.pool = nn.AvgPool2d(8, stride=1)\n",
    "        # fearute map size = [256,1,1]\n",
    "        self.fc_out = nn.Linear(256, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def get_layers(self, block, in_channels, out_channels, stride):\n",
    "        if stride == 2:\n",
    "            down_sample = True\n",
    "        else:\n",
    "            down_sample = False\n",
    "\n",
    "        layers_list = nn.ModuleList([block(in_channels, out_channels, stride, down_sample)])\n",
    "\n",
    "        for _ in range(self.num_layers - 1):\n",
    "            layers_list.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers_list)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layers_2n(x)\n",
    "        x = self.layers_4n(x)\n",
    "        x = self.layers_6n(x)\n",
    "        x = self.layers_8n(x)\n",
    "        x = self.layers_10n(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "def ResNet34_model():\n",
    "    block = ResidualBlock\n",
    "    model = ResNet(num_layers=5, block=block)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_loader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "val_batch_loader = DataLoader(val_data, batch_size, shuffle=False)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet34_model()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Start training...\n",
      "========================================\n",
      "epoch: 1 / global_steps: 57\n",
      "training dataset average loss: 1.410\n",
      "training_time: 0.95 minutes\n",
      "validation dataset accuracy: 41.54\n",
      "tensor([-1,  0,  0,  1,  2,  0,  1,  0,  2,  0,  1,  0,  2,  3,  1,  0,  0],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.8235, device='cuda:0')\n",
      "========================================\n",
      "epoch: 2 / global_steps: 114\n",
      "training dataset average loss: 1.060\n",
      "training_time: 1.96 minutes\n",
      "validation dataset accuracy: 45.70\n",
      "tensor([ 0, -2,  0,  1, -1,  0, -1, -2,  0, -3,  0,  0,  0,  2,  0,  0, -1],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.7647, device='cuda:0')\n",
      "========================================\n",
      "epoch: 3 / global_steps: 171\n",
      "training dataset average loss: 0.966\n",
      "training_time: 3.00 minutes\n",
      "validation dataset accuracy: 38.28\n",
      "tensor([ 0, -2,  0,  0,  1, -2, -1,  1,  0,  0,  2,  0,  3,  0, -1,  0,  1],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.8235, device='cuda:0')\n",
      "========================================\n",
      "epoch: 4 / global_steps: 228\n",
      "training dataset average loss: 0.937\n",
      "training_time: 4.05 minutes\n",
      "validation dataset accuracy: 40.36\n",
      "tensor([ 2,  0,  0,  1,  1,  1,  1,  0,  2, -1,  1,  0,  2,  2,  1,  0,  0],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.8824, device='cuda:0')\n",
      "========================================\n",
      "epoch: 5 / global_steps: 285\n",
      "training dataset average loss: 0.900\n",
      "training_time: 5.09 minutes\n",
      "validation dataset accuracy: 46.88\n",
      "tensor([2, 0, 0, 2, 1, 1, 1, 0, 2, 0, 1, 0, 2, 0, 1, 0, 0], device='cuda:0')\n",
      "validation my accuracy: tensor(0.7647, device='cuda:0')\n",
      "========================================\n",
      "epoch: 6 / global_steps: 342\n",
      "training dataset average loss: 0.820\n",
      "training_time: 6.10 minutes\n",
      "validation dataset accuracy: 56.68\n",
      "tensor([ 2, -1,  0,  1,  1,  0,  0,  0,  1,  0,  0,  0,  2,  3,  0,  0,  0],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.6471, device='cuda:0')\n",
      "========================================\n",
      "epoch: 7 / global_steps: 399\n",
      "training dataset average loss: 0.725\n",
      "training_time: 7.12 minutes\n",
      "validation dataset accuracy: 51.63\n",
      "tensor([ 0, -2,  0,  1,  0,  0, -1,  0,  0,  0,  0,  0,  0,  1, -1,  0,  0],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.3529, device='cuda:0')\n",
      "========================================\n",
      "epoch: 8 / global_steps: 456\n",
      "training dataset average loss: 0.786\n",
      "training_time: 8.13 minutes\n",
      "validation dataset accuracy: 39.76\n",
      "tensor([ 2, -2,  0,  0,  2,  1, -1,  0,  0,  0,  2,  0,  2,  1,  1,  0,  0],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.8235, device='cuda:0')\n",
      "========================================\n",
      "epoch: 9 / global_steps: 513\n",
      "training dataset average loss: 0.887\n",
      "training_time: 9.15 minutes\n",
      "validation dataset accuracy: 57.27\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0, -1,  1,  0,  0,  0,  0,  1,  0,  0,  0],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.1765, device='cuda:0')\n",
      "========================================\n",
      "epoch: 10 / global_steps: 570\n",
      "training dataset average loss: 0.635\n",
      "training_time: 10.18 minutes\n",
      "validation dataset accuracy: 55.49\n",
      "tensor([ 0, -2,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  1,  1, -1,  0,  0],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.4118, device='cuda:0')\n",
      "========================================\n",
      "epoch: 11 / global_steps: 627\n",
      "training dataset average loss: 0.606\n",
      "training_time: 11.27 minutes\n",
      "validation dataset accuracy: 55.79\n",
      "tensor([ 0, -1,  0,  1,  1,  0,  1,  0,  1,  0,  0,  0,  2,  1,  1,  0,  0],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.5294, device='cuda:0')\n",
      "========================================\n",
      "epoch: 12 / global_steps: 684\n",
      "training dataset average loss: 0.537\n",
      "training_time: 12.38 minutes\n",
      "validation dataset accuracy: 58.46\n",
      "tensor([ 0, -1,  0,  1,  0,  0,  0, -1,  1,  0,  0,  0,  1,  1,  1,  0,  0],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.4118, device='cuda:0')\n",
      "========================================\n",
      "epoch: 13 / global_steps: 741\n",
      "training dataset average loss: 0.510\n",
      "training_time: 13.52 minutes\n",
      "validation dataset accuracy: 57.86\n",
      "tensor([ 0, -2,  0,  1,  0,  0,  0, -1,  1,  0,  0,  0,  1,  1,  1,  0,  0],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.4706, device='cuda:0')\n",
      "========================================\n",
      "epoch: 14 / global_steps: 798\n",
      "training dataset average loss: 0.415\n",
      "training_time: 14.59 minutes\n",
      "validation dataset accuracy: 62.31\n",
      "tensor([ 0, -2,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  1,  1,  1,  0,  0],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.3529, device='cuda:0')\n",
      "========================================\n",
      "epoch: 15 / global_steps: 855\n",
      "training dataset average loss: 0.382\n",
      "training_time: 15.63 minutes\n",
      "validation dataset accuracy: 57.27\n",
      "tensor([ 0, -1,  0,  0,  1,  0,  1,  0,  1,  0,  0,  0,  1,  1,  0,  0,  0],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.3529, device='cuda:0')\n",
      "========================================\n",
      "epoch: 16 / global_steps: 912\n",
      "training dataset average loss: 0.396\n",
      "training_time: 16.68 minutes\n",
      "validation dataset accuracy: 61.72\n",
      "tensor([ 0,  0,  0,  0,  1,  0, -1,  0,  1,  0,  0,  0,  1,  1,  1,  0,  0],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.3529, device='cuda:0')\n",
      "========================================\n",
      "epoch: 17 / global_steps: 969\n",
      "training dataset average loss: 0.378\n",
      "training_time: 17.71 minutes\n",
      "validation dataset accuracy: 61.42\n",
      "tensor([ 0, -1,  0,  0,  0,  0,  0, -1,  1,  0,  0,  0,  0,  1,  1,  0,  0],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.2941, device='cuda:0')\n",
      "========================================\n",
      "epoch: 18 / global_steps: 1026\n",
      "training dataset average loss: 0.189\n",
      "training_time: 18.77 minutes\n",
      "validation dataset accuracy: 61.42\n",
      "tensor([ 0, -1,  0,  0,  1,  0,  0,  0,  1,  0,  0,  0,  1,  1,  1,  0,  0],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.3529, device='cuda:0')\n",
      "========================================\n",
      "epoch: 19 / global_steps: 1083\n",
      "training dataset average loss: 0.148\n",
      "training_time: 19.81 minutes\n",
      "validation dataset accuracy: 59.64\n",
      "tensor([ 0, -1,  0,  0,  1,  0,  1, -1,  1,  0,  0,  0,  1,  2,  1,  0,  0],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.5294, device='cuda:0')\n",
      "========================================\n",
      "epoch: 20 / global_steps: 1140\n",
      "training dataset average loss: 0.139\n",
      "training_time: 20.86 minutes\n",
      "validation dataset accuracy: 60.53\n",
      "tensor([ 0, -1,  0,  0,  0,  0, -1,  0,  1,  0, -1,  0,  1,  1,  1,  0,  0],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.4118, device='cuda:0')\n",
      "========================================\n",
      "epoch: 21 / global_steps: 1197\n",
      "training dataset average loss: 0.182\n",
      "training_time: 21.90 minutes\n",
      "validation dataset accuracy: 60.83\n",
      "tensor([ 0, -1,  0,  0,  1,  0,  1, -1,  1,  0, -1,  0,  1,  2,  1,  0,  0],\n",
      "       device='cuda:0')\n",
      "validation my accuracy: tensor(0.5882, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "highest_val_acc = 0\n",
    "loss_list = []\n",
    "val_acc_list = []\n",
    "global_steps = 0\n",
    "epoch = 0\n",
    "\n",
    "print('========================================')\n",
    "print(\"Start training...\")\n",
    "while True:\n",
    "    train_loss = 0\n",
    "    train_batch_cnt = 0\n",
    "    model.train()\n",
    "    for img, label in train_batch_loader:\n",
    "        global_steps += 1\n",
    "        # img.shape: [128,3,32,32]\n",
    "        # label.shape: [128]\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(img)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss\n",
    "        train_batch_cnt += 1\n",
    "\n",
    "        loss_list.append(loss)        \n",
    "\n",
    "        if global_steps >= 15000:\n",
    "            print(\"Training finished.\")\n",
    "            break\n",
    "        elif epoch > 20:\n",
    "            break\n",
    "        \n",
    "\n",
    "    ave_loss = train_loss / train_batch_cnt\n",
    "    ave_loss_list = []\n",
    "    ave_loss_list.append(ave_loss)\n",
    "    training_time = (time.time() - start_time) / 60\n",
    "    print('========================================')\n",
    "    print(\"epoch:\", epoch + 1, \"/ global_steps:\", global_steps)\n",
    "    print(\"training dataset average loss: %.3f\" % ave_loss)\n",
    "    print(\"training_time: %.2f minutes\" % training_time)\n",
    "\n",
    "    # validation (for early stopping)\n",
    "    correct_cnt = 0\n",
    "    model.eval()\n",
    "    for img, label in val_batch_loader:\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        pred = model.forward(img)\n",
    "        _, top_pred = torch.topk(pred, k=1, dim=-1)\n",
    "        top_pred = top_pred.squeeze(dim=1)\n",
    "        correct_cnt += int(torch.sum(top_pred == label))\n",
    "\n",
    "    val_acc = correct_cnt / len(val_data) * 100\n",
    "    print(\"validation dataset accuracy: %.2f\" % val_acc)\n",
    "    print(top_pred-label)\n",
    "    print('validation my accuracy:',abs(label - top_pred).sum()/len(label))\n",
    "    val_acc_list.append(val_acc)\n",
    "    if val_acc > highest_val_acc:\n",
    "        save_path = './' + str(epoch + 1) + '.pth'\n",
    "        # 위와 같이 저장 위치를 바꾸어 가며 각 setting의 epoch마다의 state를 저장할 것.\n",
    "        torch.save({'epoch': epoch + 1,\n",
    "                    'model_state_dict': model.state_dict()},\n",
    "                    save_path)\n",
    "        highest_val_acc = val_acc\n",
    "    epoch += 1\n",
    "    if global_steps >= 15000:\n",
    "        break\n",
    "    elif epoch > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data accuracy : 77.09\n"
     ]
    }
   ],
   "source": [
    "trans_test = transforms.Compose([transforms.Resize((128,128)),transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "test_data = torchvision.datasets.ImageFolder(root ='HRI_image', transform = trans_test )\n",
    "test_batch_loader = DataLoader(test_data, batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "checkpoint = torch.load(save_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "correct_cnt = 0\n",
    "with torch.no_grad():\n",
    "  for img, label in test_batch_loader:\n",
    "    img = img.to(device)\n",
    "    label = label.to(device)\n",
    "    pred = model.forward(img)\n",
    "    _, top_pred = torch.topk(pred, k=1, dim=-1)\n",
    "    top_pred = top_pred.squeeze(dim=1)\n",
    "    correct_cnt += int(torch.sum(top_pred == label))\n",
    "    \n",
    "test_acc = correct_cnt / len(test_data) * 100\n",
    "print(\"test data accuracy : %.2f\"%(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(ave_loss_list)\n",
    "#plt.show()\n",
    "plt.plot(val_acc_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.1820, device='cuda:0', grad_fn=<DivBackward0>)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ave_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd77d87a4de16a9f9d3defb60096616496f7169aca57ee3347a964a29a767458"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
